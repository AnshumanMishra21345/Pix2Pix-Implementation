{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LZx0SEt7w7aZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d, img_size, num_classes):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.embed = nn.Embedding(num_classes, img_size * img_size)\n",
        "        self.disc = nn.Sequential(\n",
        "            nn.Conv2d(channels_img + 1, features_d, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
        "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
        "            nn.Conv2d(features_d * 4, 1, kernel_size=4, stride=1, padding=0)\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "            nn.InstanceNorm2d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        embedding = self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size)\n",
        "        x = torch.cat([x, embedding], dim=1)\n",
        "        return self.disc(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g, num_classes, img_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.noise_dim = channels_noise\n",
        "        self.embed = nn.Embedding(num_classes, channels_noise)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            self._block(channels_noise * 2, features_g * 8, 4, 1, 0),\n",
        "            self._block(features_g * 8, features_g * 4, 4, 2, 1),\n",
        "            self._block(features_g * 4, features_g * 2, 4, 2, 1),\n",
        "            nn.ConvTranspose2d(features_g * 2, channels_img, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        batch_size = noise.shape[0]\n",
        "        noise = noise.view(batch_size, self.noise_dim, 1, 1)\n",
        "\n",
        "        embedding = self.embed(labels).view(batch_size, self.noise_dim, 1, 1)\n",
        "\n",
        "        x = torch.cat([noise, embedding], dim=1)\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "log_dir = \"./logs\"\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "writer_fake = SummaryWriter(os.path.join(log_dir, \"fake\"))\n",
        "writer_real = SummaryWriter(os.path.join(log_dir, \"real\"))\n",
        "step = 0\n",
        "\n",
        "def train_cgan(dataset_name=\"MNIST\", num_epochs=50, batch_size=128, lr=2e-4, img_size=32, num_classes=10):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,)) if dataset_name == \"MNIST\" else transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    if dataset_name == \"MNIST\":\n",
        "        dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "    elif dataset_name == \"CIFAR10\":\n",
        "        dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
        "    else:\n",
        "        raise ValueError(\"Dataset not supported.\")\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    channels_img = 1 if dataset_name == \"MNIST\" else 3\n",
        "    channels_noise = 100\n",
        "    features_g = 64\n",
        "    features_d = 64\n",
        "\n",
        "    gen = Generator(channels_noise, channels_img, features_g, num_classes, img_size).to(device)\n",
        "    disc = Discriminator(channels_img, features_d, img_size, num_classes).to(device)\n",
        "\n",
        "    opt_gen = optim.Adam(gen.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    opt_disc = optim.Adam(disc.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    fixed_noise = torch.randn(64, channels_noise).to(device)\n",
        "    fixed_labels = torch.randint(0, num_classes, (64,)).to(device)\n",
        "\n",
        "    global step\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, (real, labels) in enumerate(dataloader):\n",
        "            real, labels = real.to(device), labels.to(device)\n",
        "            batch_size = real.shape[0]\n",
        "\n",
        "            noise = torch.randn(batch_size, channels_noise).to(device)\n",
        "            fake = gen(noise, labels)\n",
        "\n",
        "            disc_real = disc(real, labels).reshape(-1)\n",
        "            loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
        "\n",
        "            disc_fake = disc(fake.detach(), labels).reshape(-1)\n",
        "            loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
        "\n",
        "            loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
        "            disc.zero_grad()\n",
        "            loss_disc.backward()\n",
        "            opt_disc.step()\n",
        "\n",
        "            output = disc(fake, labels).reshape(-1)\n",
        "            loss_gen = criterion(output, torch.ones_like(output))\n",
        "            gen.zero_grad()\n",
        "            loss_gen.backward()\n",
        "            opt_gen.step()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}] Batch {batch_idx}/{len(dataloader)} \"\n",
        "                      f\"Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n",
        "\n",
        "                writer_fake.add_scalar(\"Loss/Generator\", loss_gen.item(), global_step=step)\n",
        "                writer_real.add_scalar(\"Loss/Discriminator\", loss_disc.item(), global_step=step)\n",
        "                step += 1\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss D: {loss_disc.item():.4f}, Loss G: {loss_gen.item():.4f}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            fake = gen(fixed_noise, fixed_labels)\n",
        "            img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
        "            img_grid_real = torchvision.utils.make_grid(real[:64], normalize=True)\n",
        "\n",
        "            writer_fake.add_image(f\"{dataset_name} Fake\", img_grid_fake, global_step=epoch)\n",
        "            writer_real.add_image(f\"{dataset_name} Real\", img_grid_real, global_step=epoch)\n",
        "\n",
        "    torch.save(gen.state_dict(), \"generator.pth\")\n",
        "    torch.save(disc.state_dict(), \"discriminator.pth\")\n",
        "    print(\"Training complete. Models saved.\")\n",
        "\n",
        "train_cgan(dataset_name=\"MNIST\", num_epochs=50)\n",
        "writer_fake.close()\n",
        "writer_real.close()"
      ],
      "metadata": {
        "id": "cC36OVlkxjK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c3b20a-0c00-4955-bf4b-78271a21251c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch [1/50] Batch 0/469 Loss D: 0.7319, Loss G: 0.8717\n",
            "Epoch [1/50] Batch 100/469 Loss D: 0.8035, Loss G: 0.7991\n",
            "Epoch [1/50] Batch 200/469 Loss D: 0.3755, Loss G: 2.4367\n",
            "Epoch [1/50] Batch 300/469 Loss D: 0.3698, Loss G: 1.9489\n",
            "Epoch [1/50] Batch 400/469 Loss D: 0.3353, Loss G: 2.1972\n",
            "Epoch [1/50] Loss D: 0.3810, Loss G: 2.5593\n",
            "Epoch [2/50] Batch 0/469 Loss D: 0.4465, Loss G: 1.9349\n",
            "Epoch [2/50] Batch 100/469 Loss D: 0.3155, Loss G: 2.1350\n",
            "Epoch [2/50] Batch 200/469 Loss D: 0.2694, Loss G: 2.6079\n",
            "Epoch [2/50] Batch 300/469 Loss D: 0.2323, Loss G: 3.1206\n",
            "Epoch [2/50] Batch 400/469 Loss D: 0.2958, Loss G: 3.0966\n",
            "Epoch [2/50] Loss D: 0.2581, Loss G: 2.8493\n",
            "Epoch [3/50] Batch 0/469 Loss D: 0.2479, Loss G: 2.1854\n",
            "Epoch [3/50] Batch 100/469 Loss D: 0.3388, Loss G: 2.7881\n",
            "Epoch [3/50] Batch 200/469 Loss D: 0.6343, Loss G: 1.7116\n",
            "Epoch [3/50] Batch 300/469 Loss D: 0.3305, Loss G: 1.9741\n",
            "Epoch [3/50] Batch 400/469 Loss D: 0.2134, Loss G: 2.7528\n",
            "Epoch [3/50] Loss D: 0.1832, Loss G: 2.8335\n",
            "Epoch [4/50] Batch 0/469 Loss D: 0.2046, Loss G: 2.8745\n",
            "Epoch [4/50] Batch 100/469 Loss D: 0.3282, Loss G: 3.9350\n",
            "Epoch [4/50] Batch 200/469 Loss D: 0.2287, Loss G: 2.3951\n",
            "Epoch [4/50] Batch 300/469 Loss D: 0.2341, Loss G: 2.6479\n",
            "Epoch [4/50] Batch 400/469 Loss D: 0.1972, Loss G: 2.8444\n",
            "Epoch [4/50] Loss D: 0.2379, Loss G: 2.9817\n",
            "Epoch [5/50] Batch 0/469 Loss D: 0.2098, Loss G: 3.5903\n",
            "Epoch [5/50] Batch 100/469 Loss D: 0.2118, Loss G: 2.3150\n",
            "Epoch [5/50] Batch 200/469 Loss D: 0.2454, Loss G: 3.3477\n",
            "Epoch [5/50] Batch 300/469 Loss D: 0.2476, Loss G: 2.8768\n",
            "Epoch [5/50] Batch 400/469 Loss D: 0.2417, Loss G: 4.4876\n",
            "Epoch [5/50] Loss D: 0.1765, Loss G: 4.1884\n",
            "Epoch [6/50] Batch 0/469 Loss D: 0.3189, Loss G: 2.1957\n",
            "Epoch [6/50] Batch 100/469 Loss D: 0.2038, Loss G: 2.7397\n",
            "Epoch [6/50] Batch 200/469 Loss D: 0.3408, Loss G: 2.0867\n",
            "Epoch [6/50] Batch 300/469 Loss D: 0.1432, Loss G: 3.1254\n",
            "Epoch [6/50] Batch 400/469 Loss D: 0.2031, Loss G: 3.6931\n",
            "Epoch [6/50] Loss D: 0.3597, Loss G: 4.4822\n",
            "Epoch [7/50] Batch 0/469 Loss D: 0.4133, Loss G: 1.9895\n",
            "Epoch [7/50] Batch 100/469 Loss D: 0.2794, Loss G: 1.6701\n",
            "Epoch [7/50] Batch 200/469 Loss D: 0.2269, Loss G: 2.3972\n",
            "Epoch [7/50] Batch 300/469 Loss D: 0.1345, Loss G: 3.7329\n",
            "Epoch [7/50] Batch 400/469 Loss D: 0.2121, Loss G: 2.1556\n",
            "Epoch [7/50] Loss D: 0.1901, Loss G: 3.8712\n",
            "Epoch [8/50] Batch 0/469 Loss D: 0.1512, Loss G: 3.2739\n",
            "Epoch [8/50] Batch 100/469 Loss D: 0.1483, Loss G: 3.5121\n",
            "Epoch [8/50] Batch 200/469 Loss D: 0.1766, Loss G: 2.9005\n",
            "Epoch [8/50] Batch 300/469 Loss D: 0.2428, Loss G: 3.5185\n",
            "Epoch [8/50] Batch 400/469 Loss D: 0.2147, Loss G: 3.0094\n",
            "Epoch [8/50] Loss D: 0.1587, Loss G: 3.7641\n",
            "Epoch [9/50] Batch 0/469 Loss D: 0.0842, Loss G: 3.9982\n",
            "Epoch [9/50] Batch 100/469 Loss D: 0.1439, Loss G: 3.7506\n",
            "Epoch [9/50] Batch 200/469 Loss D: 0.2246, Loss G: 2.9613\n",
            "Epoch [9/50] Batch 300/469 Loss D: 0.2846, Loss G: 5.4697\n",
            "Epoch [9/50] Batch 400/469 Loss D: 0.2232, Loss G: 3.2766\n",
            "Epoch [9/50] Loss D: 0.1876, Loss G: 2.4048\n",
            "Epoch [10/50] Batch 0/469 Loss D: 0.1867, Loss G: 5.3354\n",
            "Epoch [10/50] Batch 100/469 Loss D: 0.2795, Loss G: 5.9753\n",
            "Epoch [10/50] Batch 200/469 Loss D: 0.1384, Loss G: 4.0992\n",
            "Epoch [10/50] Batch 300/469 Loss D: 0.2540, Loss G: 3.3852\n",
            "Epoch [10/50] Batch 400/469 Loss D: 0.1434, Loss G: 4.1263\n",
            "Epoch [10/50] Loss D: 0.1884, Loss G: 3.9885\n",
            "Epoch [11/50] Batch 0/469 Loss D: 0.2288, Loss G: 3.7043\n",
            "Epoch [11/50] Batch 100/469 Loss D: 0.2008, Loss G: 3.2550\n",
            "Epoch [11/50] Batch 200/469 Loss D: 0.1262, Loss G: 3.2439\n",
            "Epoch [11/50] Batch 300/469 Loss D: 0.1803, Loss G: 3.1712\n",
            "Epoch [11/50] Batch 400/469 Loss D: 0.2496, Loss G: 5.9753\n",
            "Epoch [11/50] Loss D: 0.0929, Loss G: 4.0198\n",
            "Epoch [12/50] Batch 0/469 Loss D: 0.1370, Loss G: 2.9939\n",
            "Epoch [12/50] Batch 100/469 Loss D: 0.1935, Loss G: 2.9083\n",
            "Epoch [12/50] Batch 200/469 Loss D: 0.1197, Loss G: 3.7144\n",
            "Epoch [12/50] Batch 300/469 Loss D: 0.2002, Loss G: 4.5833\n",
            "Epoch [12/50] Batch 400/469 Loss D: 0.2111, Loss G: 4.4123\n",
            "Epoch [12/50] Loss D: 0.1470, Loss G: 3.9458\n",
            "Epoch [13/50] Batch 0/469 Loss D: 0.1683, Loss G: 2.4981\n",
            "Epoch [13/50] Batch 100/469 Loss D: 0.1884, Loss G: 2.5646\n",
            "Epoch [13/50] Batch 200/469 Loss D: 0.1251, Loss G: 3.8288\n",
            "Epoch [13/50] Batch 300/469 Loss D: 0.2030, Loss G: 2.0307\n",
            "Epoch [13/50] Batch 400/469 Loss D: 0.1287, Loss G: 4.2878\n",
            "Epoch [13/50] Loss D: 0.0995, Loss G: 3.3365\n",
            "Epoch [14/50] Batch 0/469 Loss D: 0.0942, Loss G: 3.3696\n",
            "Epoch [14/50] Batch 100/469 Loss D: 0.2388, Loss G: 7.3180\n",
            "Epoch [14/50] Batch 200/469 Loss D: 0.1994, Loss G: 2.6988\n",
            "Epoch [14/50] Batch 300/469 Loss D: 0.1607, Loss G: 3.4348\n",
            "Epoch [14/50] Batch 400/469 Loss D: 0.1932, Loss G: 5.5648\n",
            "Epoch [14/50] Loss D: 0.1385, Loss G: 3.5918\n",
            "Epoch [15/50] Batch 0/469 Loss D: 0.0978, Loss G: 3.6917\n",
            "Epoch [15/50] Batch 100/469 Loss D: 0.2602, Loss G: 3.4094\n",
            "Epoch [15/50] Batch 200/469 Loss D: 0.0892, Loss G: 4.2018\n",
            "Epoch [15/50] Batch 300/469 Loss D: 0.3449, Loss G: 6.3695\n",
            "Epoch [15/50] Batch 400/469 Loss D: 0.0921, Loss G: 3.6292\n",
            "Epoch [15/50] Loss D: 0.1790, Loss G: 3.9630\n",
            "Epoch [16/50] Batch 0/469 Loss D: 0.1343, Loss G: 3.5450\n",
            "Epoch [16/50] Batch 100/469 Loss D: 0.1226, Loss G: 4.0114\n",
            "Epoch [16/50] Batch 200/469 Loss D: 0.1626, Loss G: 5.2591\n",
            "Epoch [16/50] Batch 300/469 Loss D: 0.1741, Loss G: 3.4595\n",
            "Epoch [16/50] Batch 400/469 Loss D: 0.0850, Loss G: 3.7008\n",
            "Epoch [16/50] Loss D: 0.1331, Loss G: 3.1558\n",
            "Epoch [17/50] Batch 0/469 Loss D: 0.1635, Loss G: 5.4045\n",
            "Epoch [17/50] Batch 100/469 Loss D: 0.1270, Loss G: 3.9622\n",
            "Epoch [17/50] Batch 200/469 Loss D: 0.2164, Loss G: 4.8728\n",
            "Epoch [17/50] Batch 300/469 Loss D: 0.1223, Loss G: 3.5960\n",
            "Epoch [17/50] Batch 400/469 Loss D: 0.1170, Loss G: 4.3494\n",
            "Epoch [17/50] Loss D: 0.1530, Loss G: 5.5417\n",
            "Epoch [18/50] Batch 0/469 Loss D: 0.2083, Loss G: 3.1049\n",
            "Epoch [18/50] Batch 100/469 Loss D: 0.1953, Loss G: 3.6692\n",
            "Epoch [18/50] Batch 200/469 Loss D: 0.1841, Loss G: 2.3349\n",
            "Epoch [18/50] Batch 300/469 Loss D: 0.1746, Loss G: 3.6545\n",
            "Epoch [18/50] Batch 400/469 Loss D: 0.1168, Loss G: 3.3917\n",
            "Epoch [18/50] Loss D: 0.2282, Loss G: 7.1212\n",
            "Epoch [19/50] Batch 0/469 Loss D: 0.1861, Loss G: 5.3687\n",
            "Epoch [19/50] Batch 100/469 Loss D: 0.0940, Loss G: 4.2947\n",
            "Epoch [19/50] Batch 200/469 Loss D: 0.1298, Loss G: 2.9023\n",
            "Epoch [19/50] Batch 300/469 Loss D: 0.1213, Loss G: 5.2507\n",
            "Epoch [19/50] Batch 400/469 Loss D: 0.4162, Loss G: 5.9646\n",
            "Epoch [19/50] Loss D: 0.3380, Loss G: 3.2061\n",
            "Epoch [20/50] Batch 0/469 Loss D: 0.1546, Loss G: 4.4021\n",
            "Epoch [20/50] Batch 100/469 Loss D: 0.1851, Loss G: 4.3224\n",
            "Epoch [20/50] Batch 200/469 Loss D: 0.0903, Loss G: 4.1838\n",
            "Epoch [20/50] Batch 300/469 Loss D: 0.0657, Loss G: 4.2242\n",
            "Epoch [20/50] Batch 400/469 Loss D: 0.1366, Loss G: 3.0053\n",
            "Epoch [20/50] Loss D: 0.1454, Loss G: 3.8427\n",
            "Epoch [21/50] Batch 0/469 Loss D: 0.1709, Loss G: 3.2471\n",
            "Epoch [21/50] Batch 100/469 Loss D: 0.1749, Loss G: 3.0985\n",
            "Epoch [21/50] Batch 200/469 Loss D: 0.1277, Loss G: 2.6859\n",
            "Epoch [21/50] Batch 300/469 Loss D: 0.1445, Loss G: 3.1287\n",
            "Epoch [21/50] Batch 400/469 Loss D: 0.1530, Loss G: 2.7825\n",
            "Epoch [21/50] Loss D: 0.1415, Loss G: 3.5892\n",
            "Epoch [22/50] Batch 0/469 Loss D: 0.1605, Loss G: 3.2427\n",
            "Epoch [22/50] Batch 100/469 Loss D: 0.2108, Loss G: 4.7734\n",
            "Epoch [22/50] Batch 200/469 Loss D: 0.1048, Loss G: 3.2501\n",
            "Epoch [22/50] Batch 300/469 Loss D: 0.1559, Loss G: 2.6970\n",
            "Epoch [22/50] Batch 400/469 Loss D: 0.0709, Loss G: 4.2493\n",
            "Epoch [22/50] Loss D: 0.2485, Loss G: 3.4526\n",
            "Epoch [23/50] Batch 0/469 Loss D: 0.1692, Loss G: 7.0855\n",
            "Epoch [23/50] Batch 100/469 Loss D: 0.1520, Loss G: 2.9630\n",
            "Epoch [23/50] Batch 200/469 Loss D: 0.0554, Loss G: 3.9677\n",
            "Epoch [23/50] Batch 300/469 Loss D: 0.1208, Loss G: 3.3034\n",
            "Epoch [23/50] Batch 400/469 Loss D: 0.1062, Loss G: 3.2742\n",
            "Epoch [23/50] Loss D: 0.0917, Loss G: 4.3980\n",
            "Epoch [24/50] Batch 0/469 Loss D: 0.2120, Loss G: 2.3042\n",
            "Epoch [24/50] Batch 100/469 Loss D: 0.1357, Loss G: 3.4584\n",
            "Epoch [24/50] Batch 200/469 Loss D: 0.1125, Loss G: 5.2079\n",
            "Epoch [24/50] Batch 300/469 Loss D: 0.1208, Loss G: 3.3587\n",
            "Epoch [24/50] Batch 400/469 Loss D: 0.1255, Loss G: 3.7723\n",
            "Epoch [24/50] Loss D: 0.1039, Loss G: 3.5831\n",
            "Epoch [25/50] Batch 0/469 Loss D: 0.0784, Loss G: 3.8190\n",
            "Epoch [25/50] Batch 100/469 Loss D: 0.0758, Loss G: 4.0383\n",
            "Epoch [25/50] Batch 200/469 Loss D: 0.1583, Loss G: 4.0822\n",
            "Epoch [25/50] Batch 300/469 Loss D: 0.0841, Loss G: 3.5912\n",
            "Epoch [25/50] Batch 400/469 Loss D: 0.1937, Loss G: 3.0522\n",
            "Epoch [25/50] Loss D: 0.0736, Loss G: 4.6374\n",
            "Epoch [26/50] Batch 0/469 Loss D: 0.1307, Loss G: 3.4136\n",
            "Epoch [26/50] Batch 100/469 Loss D: 0.1224, Loss G: 3.4464\n",
            "Epoch [26/50] Batch 200/469 Loss D: 0.2212, Loss G: 2.8484\n",
            "Epoch [26/50] Batch 300/469 Loss D: 0.0756, Loss G: 3.5610\n",
            "Epoch [26/50] Batch 400/469 Loss D: 0.1555, Loss G: 7.2165\n",
            "Epoch [26/50] Loss D: 0.1871, Loss G: 3.3268\n",
            "Epoch [27/50] Batch 0/469 Loss D: 0.1001, Loss G: 4.7011\n",
            "Epoch [27/50] Batch 100/469 Loss D: 0.1639, Loss G: 3.5437\n",
            "Epoch [27/50] Batch 200/469 Loss D: 0.0826, Loss G: 4.5478\n",
            "Epoch [27/50] Batch 300/469 Loss D: 0.0563, Loss G: 4.4226\n",
            "Epoch [27/50] Batch 400/469 Loss D: 0.1494, Loss G: 4.1707\n",
            "Epoch [27/50] Loss D: 0.0519, Loss G: 4.6700\n",
            "Epoch [28/50] Batch 0/469 Loss D: 0.0833, Loss G: 3.0810\n",
            "Epoch [28/50] Batch 100/469 Loss D: 0.0850, Loss G: 3.4219\n",
            "Epoch [28/50] Batch 200/469 Loss D: 0.2655, Loss G: 3.6600\n",
            "Epoch [28/50] Batch 300/469 Loss D: 0.0869, Loss G: 4.4236\n",
            "Epoch [28/50] Batch 400/469 Loss D: 0.0674, Loss G: 5.3578\n",
            "Epoch [28/50] Loss D: 0.0615, Loss G: 4.3121\n",
            "Epoch [29/50] Batch 0/469 Loss D: 0.0744, Loss G: 3.7445\n",
            "Epoch [29/50] Batch 100/469 Loss D: 0.0766, Loss G: 4.1677\n",
            "Epoch [29/50] Batch 200/469 Loss D: 0.1097, Loss G: 3.5560\n",
            "Epoch [29/50] Batch 300/469 Loss D: 0.0430, Loss G: 4.3168\n",
            "Epoch [29/50] Batch 400/469 Loss D: 0.0639, Loss G: 5.4269\n",
            "Epoch [29/50] Loss D: 0.1911, Loss G: 5.1386\n",
            "Epoch [30/50] Batch 0/469 Loss D: 0.1773, Loss G: 3.7512\n",
            "Epoch [30/50] Batch 100/469 Loss D: 0.0497, Loss G: 4.6554\n",
            "Epoch [30/50] Batch 200/469 Loss D: 0.0915, Loss G: 3.8547\n",
            "Epoch [30/50] Batch 300/469 Loss D: 0.1086, Loss G: 5.3462\n",
            "Epoch [30/50] Batch 400/469 Loss D: 0.0740, Loss G: 3.9910\n",
            "Epoch [30/50] Loss D: 0.0835, Loss G: 4.1619\n",
            "Epoch [31/50] Batch 0/469 Loss D: 0.0936, Loss G: 3.5297\n",
            "Epoch [31/50] Batch 100/469 Loss D: 0.0635, Loss G: 4.5096\n",
            "Epoch [31/50] Batch 200/469 Loss D: 0.0289, Loss G: 4.8196\n",
            "Epoch [31/50] Batch 300/469 Loss D: 0.0493, Loss G: 4.4219\n",
            "Epoch [31/50] Batch 400/469 Loss D: 0.1622, Loss G: 6.4615\n",
            "Epoch [31/50] Loss D: 0.0416, Loss G: 5.0265\n",
            "Epoch [32/50] Batch 0/469 Loss D: 0.0855, Loss G: 3.6810\n",
            "Epoch [32/50] Batch 100/469 Loss D: 0.0809, Loss G: 4.3348\n",
            "Epoch [32/50] Batch 200/469 Loss D: 0.0927, Loss G: 4.0336\n",
            "Epoch [32/50] Batch 300/469 Loss D: 0.1074, Loss G: 4.7851\n",
            "Epoch [32/50] Batch 400/469 Loss D: 0.3307, Loss G: 5.6879\n",
            "Epoch [32/50] Loss D: 0.0594, Loss G: 4.2561\n",
            "Epoch [33/50] Batch 0/469 Loss D: 0.0733, Loss G: 4.1090\n",
            "Epoch [33/50] Batch 100/469 Loss D: 0.0780, Loss G: 4.2317\n",
            "Epoch [33/50] Batch 200/469 Loss D: 0.0419, Loss G: 4.8529\n",
            "Epoch [33/50] Batch 300/469 Loss D: 0.0853, Loss G: 4.3027\n",
            "Epoch [33/50] Batch 400/469 Loss D: 0.0426, Loss G: 5.6402\n",
            "Epoch [33/50] Loss D: 0.1216, Loss G: 4.7201\n",
            "Epoch [34/50] Batch 0/469 Loss D: 0.0947, Loss G: 4.6885\n",
            "Epoch [34/50] Batch 100/469 Loss D: 0.0983, Loss G: 3.7546\n",
            "Epoch [34/50] Batch 200/469 Loss D: 0.0484, Loss G: 4.1624\n",
            "Epoch [34/50] Batch 300/469 Loss D: 0.0563, Loss G: 4.6070\n",
            "Epoch [34/50] Batch 400/469 Loss D: 0.0272, Loss G: 4.8922\n",
            "Epoch [34/50] Loss D: 0.1245, Loss G: 5.3304\n",
            "Epoch [35/50] Batch 0/469 Loss D: 0.0966, Loss G: 5.2170\n",
            "Epoch [35/50] Batch 100/469 Loss D: 0.0481, Loss G: 5.8316\n",
            "Epoch [35/50] Batch 200/469 Loss D: 0.1670, Loss G: 9.2787\n",
            "Epoch [35/50] Batch 300/469 Loss D: 0.0293, Loss G: 4.3368\n",
            "Epoch [35/50] Batch 400/469 Loss D: 0.0839, Loss G: 3.2743\n",
            "Epoch [35/50] Loss D: 0.0354, Loss G: 5.4439\n",
            "Epoch [36/50] Batch 0/469 Loss D: 0.0571, Loss G: 4.6551\n",
            "Epoch [36/50] Batch 100/469 Loss D: 0.1329, Loss G: 3.4181\n",
            "Epoch [36/50] Batch 200/469 Loss D: 0.2035, Loss G: 4.3351\n",
            "Epoch [36/50] Batch 300/469 Loss D: 0.1398, Loss G: 6.0145\n",
            "Epoch [36/50] Batch 400/469 Loss D: 0.0350, Loss G: 5.2209\n",
            "Epoch [36/50] Loss D: 0.0461, Loss G: 4.4471\n",
            "Epoch [37/50] Batch 0/469 Loss D: 0.0503, Loss G: 4.6300\n",
            "Epoch [37/50] Batch 100/469 Loss D: 0.2415, Loss G: 6.0017\n",
            "Epoch [37/50] Batch 200/469 Loss D: 0.0739, Loss G: 4.2452\n",
            "Epoch [37/50] Batch 300/469 Loss D: 0.0235, Loss G: 4.9683\n",
            "Epoch [37/50] Batch 400/469 Loss D: 0.1375, Loss G: 7.7812\n",
            "Epoch [37/50] Loss D: 0.0622, Loss G: 3.6121\n",
            "Epoch [38/50] Batch 0/469 Loss D: 0.0849, Loss G: 6.6794\n",
            "Epoch [38/50] Batch 100/469 Loss D: 0.0240, Loss G: 4.8280\n",
            "Epoch [38/50] Batch 200/469 Loss D: 0.0217, Loss G: 4.9168\n",
            "Epoch [38/50] Batch 300/469 Loss D: 0.0667, Loss G: 4.7733\n",
            "Epoch [38/50] Batch 400/469 Loss D: 0.0361, Loss G: 4.8908\n",
            "Epoch [38/50] Loss D: 0.0556, Loss G: 4.5314\n",
            "Epoch [39/50] Batch 0/469 Loss D: 0.0550, Loss G: 4.2215\n",
            "Epoch [39/50] Batch 100/469 Loss D: 0.0459, Loss G: 4.2083\n",
            "Epoch [39/50] Batch 200/469 Loss D: 0.0552, Loss G: 4.8517\n",
            "Epoch [39/50] Batch 300/469 Loss D: 0.0758, Loss G: 5.0266\n",
            "Epoch [39/50] Batch 400/469 Loss D: 0.0356, Loss G: 4.8373\n",
            "Epoch [39/50] Loss D: 0.1310, Loss G: 4.2659\n",
            "Epoch [40/50] Batch 0/469 Loss D: 0.0729, Loss G: 6.1008\n",
            "Epoch [40/50] Batch 100/469 Loss D: 0.0210, Loss G: 6.1247\n",
            "Epoch [40/50] Batch 200/469 Loss D: 0.1123, Loss G: 6.0245\n",
            "Epoch [40/50] Batch 300/469 Loss D: 0.0412, Loss G: 5.1941\n",
            "Epoch [40/50] Batch 400/469 Loss D: 0.0394, Loss G: 4.5793\n",
            "Epoch [40/50] Loss D: 0.0663, Loss G: 3.9791\n",
            "Epoch [41/50] Batch 0/469 Loss D: 0.0681, Loss G: 5.5763\n",
            "Epoch [41/50] Batch 100/469 Loss D: 0.0590, Loss G: 4.0711\n",
            "Epoch [41/50] Batch 200/469 Loss D: 0.0574, Loss G: 7.5465\n",
            "Epoch [41/50] Batch 300/469 Loss D: 0.0152, Loss G: 5.0484\n",
            "Epoch [41/50] Batch 400/469 Loss D: 0.0734, Loss G: 4.1638\n",
            "Epoch [41/50] Loss D: 0.0341, Loss G: 4.8913\n",
            "Epoch [42/50] Batch 0/469 Loss D: 0.0888, Loss G: 3.1785\n",
            "Epoch [42/50] Batch 100/469 Loss D: 0.1006, Loss G: 6.8941\n",
            "Epoch [42/50] Batch 200/469 Loss D: 0.0370, Loss G: 4.5113\n",
            "Epoch [42/50] Batch 300/469 Loss D: 0.0494, Loss G: 4.6619\n",
            "Epoch [42/50] Batch 400/469 Loss D: 0.0261, Loss G: 6.8532\n",
            "Epoch [42/50] Loss D: 0.0805, Loss G: 5.2540\n",
            "Epoch [43/50] Batch 0/469 Loss D: 0.0265, Loss G: 5.9540\n",
            "Epoch [43/50] Batch 100/469 Loss D: 0.2481, Loss G: 2.4340\n",
            "Epoch [43/50] Batch 200/469 Loss D: 0.0421, Loss G: 5.8801\n",
            "Epoch [43/50] Batch 300/469 Loss D: 0.1105, Loss G: 3.7350\n",
            "Epoch [43/50] Batch 400/469 Loss D: 0.0436, Loss G: 5.7064\n",
            "Epoch [43/50] Loss D: 0.0283, Loss G: 4.7530\n",
            "Epoch [44/50] Batch 0/469 Loss D: 0.0525, Loss G: 4.2374\n",
            "Epoch [44/50] Batch 100/469 Loss D: 0.0555, Loss G: 5.8904\n",
            "Epoch [44/50] Batch 200/469 Loss D: 0.0417, Loss G: 4.3463\n",
            "Epoch [44/50] Batch 300/469 Loss D: 0.0224, Loss G: 4.6667\n",
            "Epoch [44/50] Batch 400/469 Loss D: 0.0456, Loss G: 4.9176\n",
            "Epoch [44/50] Loss D: 0.0137, Loss G: 5.1607\n",
            "Epoch [45/50] Batch 0/469 Loss D: 0.0424, Loss G: 4.7898\n",
            "Epoch [45/50] Batch 100/469 Loss D: 0.0212, Loss G: 5.9093\n",
            "Epoch [45/50] Batch 200/469 Loss D: 0.0310, Loss G: 4.5934\n",
            "Epoch [45/50] Batch 300/469 Loss D: 0.0648, Loss G: 4.0870\n",
            "Epoch [45/50] Batch 400/469 Loss D: 0.2140, Loss G: 16.0723\n",
            "Epoch [45/50] Loss D: 0.1090, Loss G: 4.1358\n",
            "Epoch [46/50] Batch 0/469 Loss D: 0.0647, Loss G: 5.4588\n",
            "Epoch [46/50] Batch 100/469 Loss D: 0.0553, Loss G: 3.7265\n",
            "Epoch [46/50] Batch 200/469 Loss D: 0.0217, Loss G: 4.4684\n",
            "Epoch [46/50] Batch 300/469 Loss D: 0.0385, Loss G: 6.2938\n",
            "Epoch [46/50] Batch 400/469 Loss D: 0.0219, Loss G: 5.3817\n",
            "Epoch [46/50] Loss D: 0.0281, Loss G: 6.5756\n",
            "Epoch [47/50] Batch 0/469 Loss D: 0.0762, Loss G: 5.2009\n",
            "Epoch [47/50] Batch 100/469 Loss D: 0.0314, Loss G: 5.8832\n",
            "Epoch [47/50] Batch 200/469 Loss D: 0.0212, Loss G: 5.0740\n",
            "Epoch [47/50] Batch 300/469 Loss D: 0.2576, Loss G: 6.6290\n",
            "Epoch [47/50] Batch 400/469 Loss D: 0.0810, Loss G: 4.5455\n",
            "Epoch [47/50] Loss D: 0.0579, Loss G: 4.1710\n",
            "Epoch [48/50] Batch 0/469 Loss D: 0.0623, Loss G: 5.4920\n",
            "Epoch [48/50] Batch 100/469 Loss D: 0.0174, Loss G: 5.3434\n",
            "Epoch [48/50] Batch 200/469 Loss D: 0.0715, Loss G: 4.6214\n",
            "Epoch [48/50] Batch 300/469 Loss D: 0.0285, Loss G: 5.4276\n",
            "Epoch [48/50] Batch 400/469 Loss D: 0.1429, Loss G: 10.4920\n",
            "Epoch [48/50] Loss D: 0.1425, Loss G: 13.0555\n",
            "Epoch [49/50] Batch 0/469 Loss D: 0.1374, Loss G: 12.2871\n",
            "Epoch [49/50] Batch 100/469 Loss D: 0.0772, Loss G: 7.7683\n",
            "Epoch [49/50] Batch 200/469 Loss D: 0.0290, Loss G: 4.9443\n",
            "Epoch [49/50] Batch 300/469 Loss D: 0.0532, Loss G: 4.2007\n",
            "Epoch [49/50] Batch 400/469 Loss D: 0.0278, Loss G: 5.1395\n",
            "Epoch [49/50] Loss D: 0.0415, Loss G: 6.1786\n",
            "Epoch [50/50] Batch 0/469 Loss D: 0.1478, Loss G: 6.2186\n",
            "Epoch [50/50] Batch 100/469 Loss D: 0.0830, Loss G: 5.7289\n",
            "Epoch [50/50] Batch 200/469 Loss D: 0.0350, Loss G: 5.0726\n",
            "Epoch [50/50] Batch 300/469 Loss D: 0.2716, Loss G: 5.6985\n",
            "Epoch [50/50] Batch 400/469 Loss D: 0.0575, Loss G: 4.0525\n",
            "Epoch [50/50] Loss D: 0.0712, Loss G: 3.4199\n",
            "Training complete. Models saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V-coKGfML5nc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}